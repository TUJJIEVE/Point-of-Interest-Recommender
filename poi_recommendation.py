# -*- coding: utf-8 -*-
"""poi_recommendation

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xKWe2u8WGzd0Smtf3-3QHh4d0YkQdvHk

# TIME AWARE POI RECOMMENDATION SYSTEM 
    Project for CS6670
    
    
BY

*   CS16BTECH11039
*   CS16BTECH11029
*   CS16BTECH11034
*   CS16BTECH11002

# For GOOGLE COLAB
"""

! pip3 install -U -q PyDrive

! pip3 install h5py

import numpy as np
import sklearn as sk
from sklearn.metrics.pairwise import cosine_similarity
import tensorflow as tf
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
from google.colab import files
from scipy import sparse
from math import sqrt
import math
from scipy.sparse import *

"""#### click link and then sign in using your google email"""

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

"""### For downloading and extracting the data set see in the left pane of window for the dataset"""

download = drive.CreateFile({'id': '1_p8KruCi1zMOXl7phAt6FY0L16b8kpde'})
download.GetContentFile('poidata.tar')

download = drive.CreateFile({'id': '1X3gQmWBc4vzf5udsPrqc0XISFZROmK2W'})
download.GetContentFile('simi.csv')

download = drive.CreateFile({'id': '1y2MaxDjIplMtRaWy-7dG9m47VU2ILiRx'})
download.GetContentFile('wuv.csv')

download = drive.CreateFile({'id':'1HcdMaID94WZSKslBKLbZ5clwWe3GYD9b'})
download.GetContentFile('hi.tar.gz')

download = drive.CreateFile({'id':'1HshGNzhNOZEXfDN0QBSKSyMOQCb-KXYZ'})
download.GetContentFile('cult.tar.gz')

! pwd

! tar -xvzf cult.tar.gz

download = drive.CreateFile({'id':'11W8EVfbBtMW8TajayNKqf8JTa4_s2pOg'})
download.GetContentFile('hi')

! tar -xvzf hi.tar.gz

! tar -xvf poidata.tar
! echo $?

"""# Pre Process"""

class preProcessPOI:
    
    def __init__(self,fi1,fi2,fi3):
        self.train = open(fi1,"r")
        self.tune = open(fi2,"r")
        self.test = open(fi3,"r")
        count = 0  
        user = []
        loc = []
        for line in self.train:
            l = line.split('\t')    
            user.append(l[0].strip())
            loc.append(l[1].strip())
            count+=1
        self.numUsers = len(set(user))
        self.numLoc = len(set(loc))
        self.numTimeSlots = 24
        self.K = 0
        self.alpha = 0
        self.beta = 0
        self.A = 0
        
        self.data = np.zeros((2321,5596,24))
        
        self.locIdToCoord = np.zeros((self.numLoc,2),dtype = np.float)
        self.timeSimi = np.zeros((self.numTimeSlots,self.numTimeSlots),dtype = np.float)
        self.tot_check_ins = np.zeros((self.numLoc,),dtype = np.int32)
        self.tot_check_ins_time = np.zeros((self.numLoc,self.numTimeSlots),dtype = np.int32)
        self.dateDict = {}
        
    def readFile(self):
        self.train.seek(0)
        dateDic = {}
        lineNo = 0
        for line in self.train:
            line = line.strip()
            arr = line.split('\t')
            locId =int (arr[1].split('_')[1])
            coord  = (arr[2].split(','))
            userId = int (arr[0].split('_')[1])
            timeSlot = int (arr[3].split(':')[0]) 
                            
            noMins = arr[3]
            dateId = int(arr[4])
            

            if dateId in dateDic :
                if userId in dateDic[dateId] :
                    if noMins in dateDic[dateId][userId] :
                        dateDic[dateId][userId][noMins].append(locId )
                    else:
                        dateDic[dateId][userId][noMins] = [locId]     
                else :
                    dateDic[dateId][userId] = {}
                    dateDic[dateId][userId][noMins] = [locId]
            else :
                dateDic[dateId] = {}
                dateDic[dateId][userId] = {}
                dateDic[dateId][userId][noMins] = [locId]
                
            
            lineNo+=1
            self.locIdToCoord[locId,0] = float (coord[0])
            self.locIdToCoord[locId,1] = float (coord[1])
            self.data[userId,locId,timeSlot] = 1
            
            self.tot_check_ins[locId]+=1
            self.tot_check_ins_time[locId,timeSlot] +=1
        
        dateDic2 = {}
        for x in dateDic :
            print(x)
            dateDic2[x] = {}
            
            temp = {}
            tempDic = dateDic[x]
            print(dateDic[x])           
            print(tempDic)
            for user in tempDic :
                tempDic2 = tempDic[user]
                tempsortedKeys = list(tempDic2)
                sortedKeys = sorted(tempsortedKeys)
                sortedData = []
                for y in sortedKeys :
                    sortedData.extend(tempDic2[y])
                temp[user] = sortedData
            print(temp)
            dateDic2[x] = temp

        print(dateDic2)
        self.dateDict = dateDic2
    def getCoord(self,loc):
        return self.locIdToCoord[loc][0],self.locIdToCoord[loc][1]
            
    def createCheckInVector(self,userID,timeSlot):
            temp = np.zeros((self.numLoc,), dtype = np.float)         
            temp[self.data[userID,:,timeSlot] == 1] = 1.0
            return temp

"""### FOR PRE PROCESSING THE DATA"""

poiData = preProcessPOI("/content/home/ujjieve/Desktop/poiRec/poidata/Foursquare/train.txt","/content/home/ujjieve/Desktop/poiRec/poidata/Foursquare/tune.txt","/content/home/ujjieve/Desktop/poiRec/poidata/Foursquare/test.txt")
poiData.readFile()
print (poiData.numLoc,poiData.numUsers)

"""# Temporal Influence

## For computing the time similarity
"""

### For getting rho_t_t' given in formula of section 3.3 of pdf 

def compute_time_simi(poi):
   
    for i in range(poiData.numTimeSlots):
        for j in range(i, poiData.numTimeSlots):
            if i==j:
                poiData.timeSimi[i,j] = 1.0
                continue
            sum = 0.0
            for user in range(poiData.numUsers):
                ci = poiData.createCheckInVector(user, i)
                cj = poiData.createCheckInVector(user, j)
                sum += cosine_similarity(ci.reshape(1,-1),cj.reshape(1,-1))[0]
            poiData.timeSimi[i,j] = poiData.timeSimi[j,i] = sum / poiData.numUsers
        print(i)

## TO compute the rho
# %time compute_time_simi(poiData)

np.savetxt('simi.csv',poiData.timeSimi,delimiter = ',')

print(poiData.timeSimi)

timeSimi = np.genfromtxt('simi.csv',delimiter = ',')

print(timeSimi)

wuv_matrix = np.genfromtxt('wuv.csv',delimiter = ',')

print(wuv_matrix)

"""### For getting the UTP cube matrix with time smoothing enhancement"""

## for getting the matrix cutl  given in section 3.3 of pdf 
def get_mcutl(poi,timeSimi):
    mcutl = np.zeros((poi.numUsers,poi.numLoc,poi.numTimeSlots), dtype = np.float)
    x = np.sum(timeSimi,axis = 1)
    
    for u in range(poi.numUsers):
        hi = np.genfromtxt('cutl' + str(u) + '.csv',delimiter = ',')
        mcutl[u] =  ( np.dot( hi ,timeSimi )) / x
        
    
    return mcutl

# %time mcutl = get_mcutl(poiData,timeSimi)

print(mcutl)

with open('mcutl.txt', 'w') as outfile:
    # I'm writing a header here just for the sake of readability
    # Any line starting with "#" will be ignored by numpy.loadtxt
    outfile.write('# Array shape: {0}\n'.format(mcutl.shape))

    # Iterating through a ndimensional array produces slices along
    # the last axis. This is equivalent to data[i,:,:] in this case
    for data_slice in mcutl:

        # The formatting string indicates that I'm writing out
        # the values in left-justified columns 7 characters in width
        # with 2 decimal places.  
        np.savetxt(outfile, data_slice, fmt='%s')

        # Writing out a break to indicate different slices...
        outfile.write('# New slice\n')

"""### For getting the user - user similarity matrix with time smoothing

#### To ge the user- user similarity given two users
"""

### for gettin the matrix wuv given in section 3.3 of pdf
def get_wuv(poi,mcutl,u,v):
    z = (mcutl[u] * mcutl[v]).sum(axis = 1).sum()
    x =  np.square(mcutl[u]).sum(axis = 1).sum(axis = 0)
    y =  np.square(mcutl[v]).sum(axis = 1).sum(axis = 0)
    return z/(math.sqrt(x) * math.sqrt(y))

# %time x= get_wuv(poiData,mcutl,0,12)
print(x)

"""#### To get the complete w_uv matrix"""

def get_wuv_matrix(poi,mcutl):
    wuv = np.zeros(shape = (poi.numUsers,poi.numUsers), dtype = np.float)
    for i in range(poi.numUsers):
        for j in range(i+1,poi.numUsers):
            wuv[i,j] = get_wuv(poi,mcutl,i,j)
        #print(i)
    return wuv

# %time wuv_matrix = get_wuv_matrix(poiData,mcutl)

for i in range(poiData.numUsers):
    wuv_matrix[i,i] = 1.
print(wuv_matrix)

"""## For getting the temporal influence from the train data"""

## for getting the final temporal influence given in section 3.3 
def get_temporal_inf(poi,mcutl,wuv_matrix,timeSimi):
    h = np.matmul(mcutl,timeSimi)
    prediction  = np.zeros(shape = (poi.numUsers,poi.numLoc,poi.numTimeSlots), dtype = np.float)
    for v in range(poi.numUsers):
        prediction[v] = np.sum( h * (wuv_matrix[v].reshape((len(wuv_matrix[v]),1,1))) , axis = 0)  
    return prediction

# %time z = get_temporal_inf(poiData,mcutl,wuv_matrix,timeSimi)
print(z)

! mkdir ujjieve

for u in range(poiData.numUsers):
    np.savetxt('/content/ujjieve/pred'+str(u)+'.csv',z[u],delimiter = ',')

"""# Spatial Influnce

#### For getting the distance between the two earth coordinates
"""

## Used for getting spatial influence sectino 4.1
def get_distance(coord1,coord2):
    
    lon1,lat1=coord1
    lon2,lat2=coord2

    R=6371000                               # radius of Earth in meters
    phi_1=math.radians(lat1)
    phi_2=math.radians(lat2)

    delta_phi=math.radians(lat2-lat1)
    delta_lambda=math.radians(lon2-lon1)

    a=math.sin(delta_phi/2.0)**2+\
       math.cos(phi_1)*math.cos(phi_2)*\
       math.sin(delta_lambda/2.0)**2
    c=2*math.atan2(math.sqrt(a),math.sqrt(1-a))
    return R*c / 1000.0

print ( get_distance( poiData.getCoord(0),poiData.getCoord(1) ) )

"""#### For getting the distances between every pair of location"""

def getAllDistances(poi):
  nL = poiData.numLoc
#   nL = 2
  x = np.array([get_distance(poi.locIdToCoord[i],poi.locIdToCoord[j]) for i in range(nL) for j in range(i+1,nL)])
  tri = np.zeros((nL, nL))
  tri[np.triu_indices(nL, 1)] = x
  return tri + tri.T

"""### For getting the willingness from the train data"""

def getWillingness(poi):
    res =  {}
    temp = poi.dateDict
    for x in temp :
        tempx = temp[x]
        for u in tempx :
            tempxu = tempx[u]
            for loc1Ind in  range(len(tempxu) - 1 ) :
                loc2Ind = loc1Ind + 1
                loc1 = tempxu[loc1Ind]
                loc2 = tempxu[loc2Ind]
                if loc1 == loc2 :
                    continue
                coord1 = (poi.locIdToCoord[loc1][0],poi.locIdToCoord[loc1][1])
                coord2 = (poi.locIdToCoord[loc2][0],poi.locIdToCoord[loc2][1])
                distance =  get_distance(coord1,coord2)
                if distance > int(distance) + 0.5:
                    key = int(distance) + 1
                else:
                    key = int(distance) + 0.5
                
                if key in res:
                    res[key] +=1
                else:
                    res[key] = 2
    return res

willing= getWillingness(poiData)
print(willing)

"""#### Converting it into log function"""

distances = list(willing)
print(distances)
print(np.log(distances))
will = [ willing[x] for x in distances]
print (will)
print (np.log(will))
log_will = np.log(will)
log_dist = np.log(distances)

"""#### For Computing A and K parameters using Least squares regression method"""

from sklearn.linear_model import LinearRegression
reg = LinearRegression().fit(log_dist.reshape(-1,1), log_will)
print (reg.coef_)
print(reg.intercept_)
poiData.K = reg.coef_
poiData.A = reg.intercept_

"""#### For getting the willingness given the distance after obtaining the parameters"""

def willingness(dist,poi):
    ## Tune a and k using
    if dist > int(dist) + 0.5:
        key = int(dist) + 1
    else:
        key = int(dist) + 0.5
    return poi.A*(key**poi.K)

print(poiData.K)

"""### For getting the probability of moving to a destination given an origin"""

def prob_loc(loc1,loc2,poi):
    coord1 = poi.getCoord(loc1)
    coord2 = poi.getCoord(loc2)
    wil = willingness(get_distance(coord1,coord2),poi)
    tot_wil = 0.0
    x = [ willingness(get_distance(coord1,poi.getCoord(t)),poi) for t in range(poi.numLoc)]
    x[loc1]  = 0.0
    tot_wil = np.sum(x)
    return wil / tot_wil

print ( get_distance(poiData.getCoord(0),poiData.getCoord(1)) )
# %time x  = prob_loc(0,1,poiData) 
print(x)

"""### For getting the matrix of above"""

def getAllProbLoc(poi ):
  allDis = getAllDistances(poiData)
  wi = np.multiply(np.power(np.rint(allDis),poi.K),poi.A)
  x , y = np.where(wi == np.inf)
  wi[x,y] = 0
  sumWi = np.sum(wi,axis=1)
  temp = sumWi - wi.T 
  plL = np.divide( wi ,temp)
  plL[x,y] = 1
  return plL

# %time conditionalProb = getAllProbLoc(poiData)

"""### For getting the prior probability using temporal popularity"""

## sectino 4.2 for getting p_t(l)
def get_prior_prob(loc,poi,time_slot):
    long_term_pop = poi.tot_check_ins[loc] / np.sum(poi.tot_check_ins)
    temporal_pop = poi.tot_check_ins_time[loc,time_slot] / np.sum(poi.tot_check_ins_time[:,time_slot])
    return (poi.beta * temporal_pop)  + ( ( 1- poi.beta ) * long_term_pop )

poiData.beta = 0.7
# %time x = get_prior_prob(0,poiData,8)
print (get_prior_prob(0,poiData,8))

"""## For getting the spatial Influence using temporal popularity"""

## For getting the final spatial influence cutl given in section 4.3
def get_spatial_inf(poi,user,time,loc):
    x = np.zeros((poi.numLoc,))
    for t in range(poi.numTimeSlots):
        y = poi.createCheckInVector(user,t)
        x = np.logical_or(y,x)
    l = [t for t in range(poi.numLoc)]
    l = np.array(l)
    loc_hist = l[x == True]
    post = [prob_loc(loc,t,poi) for t in loc_hist]

    return get_prior_prob(loc,poi,time) * np.prod(post)

# %time spa = get_spatial_inf(poiData,243,8,22)

poiData.beta = 0.5
# %time print (get_prior_prob(0,poiData,8))
def getAllPriorProb(poi):
  return np.array([get_prior_prob(l,poi,t) for l in range(poi.numLoc) for t in range(poi.numTimeSlots)]).reshape((poi.numLoc,poi.numTimeSlots))

# %time allPriorProb = getAllPriorProb(poiData)

"""### For computing the UTP cube incorporating the spatial influence"""

# poi , condProb(loc x loc) , allPrior(loc x timeSlot)
def getMLEScore(poi , condProb):
  uL = (np.sum(poi.data,axis=2) > 0)*1
  print(uL.shape)
  pUL = np.ones((poi.numUsers,poi.numLoc))
  for i in range(poi.numUsers):
    for j in range(poi.numLoc):
      temp = np.array(uL[i,:] * condProb[j,:])
      temp[temp == 0] = 1
      pUL[i][j] = np.prod(temp)
  return pUL

# %time sp = getMLEScore(poiData,conditionalProb)

def getSpatialScore(mle , prior ,timeSlots):
  finalScore = []
  for t in range(timeSlots) :
    finalScore.append(np.multiply(mle,prior[:,t]))
  return np.array(finalScore)

print(allPriorProb.shape)
# %time cULTspatial = getSpatialScore(sp,allPriorProb,24)

nU = 2321 
nL = 5596
nT = 24
prediction  = np.zeros(shape = (nU,nL,nT), dtype = np.float)

for u in range(nU):
    prediction[u] = np.genfromtxt('content/ujjieve/pred'+str(u)+'.csv',delimiter = ',')

print(prediction)

spatial  = np.zeros(shape = (nU,nL,nT), dtype = np.float)
for u in range(nU):
    spatial[u] = np.genfromtxt('content/cult/cULTspatial'+str(u)+'.csv',delimiter = ',')

# for u in range(poiData.numUsers):
#     prediction[u] = np.genfromtxt('content/ujjieve/pred'+str(u)+'.csv',delimiter = ',')

"""# For getting the combined UTP cube incorporating both temporal and spatial influence"""

def get_Combined(cULT , cULP , alpha):
    minCULT = np.amin(cULT)
    maxCULT = np.amax(cULT)
    minCULP = np.amin(cULP)
    maxCULP = np.amax(cULP)
    return (alpha * ((cULT-minCULT)/(maxCULT-minCULT)) +  ((1-alpha) * ((cULP-minCULP)/(maxCULP-minCULP))))

total = get_Combined(prediction,spatial,.2)

! mkdir final3

for u in range(nU):
  np.savetxt("/content/final3/final"+str(u)+".csv",total[u],delimiter=",")

! tar -czvf /content/final1.tar.gz /content/final1/

total = np.zeros(shape = (nU,nL,nT),dtype = np.float)
for u in range(nU):
    total[u] = np.genfromtxt('final3/final'+str(u)+'.csv',delimiter = ',')

"""# For getting Computing the Precision and recall given the test data and the UTP cube"""

from sklearn.metrics import confusion_matrix

def getPrecisionAndRecall(yTrue , yPred ):
    yTrue = yTrue.flatten()
    yPred = yPred.flatten()
    tn, fp, fn, tp = confusion_matrix(yTrue,yPred).ravel()
    precs = tp / (tp + fp) ;
    recal = tp / (tp + fn) ;
    return precs , recal ;

def getCombinedPrecisionRecall(finalScore , groundT , k , poi):
    #k is max allowed predictions for each user
    yP = np.zeros(groundT.shape)
    nU = poi.numUsers
    nT = poi.numTimeSlots
    nL = poi.numLoc
    print(finalScore)
    print(groundT)
    if finalScore.shape != groundT.shape :
        return -1 , -1
    for u in range(nU):
        for t in range(nT):
            temp = finalScore[u,:,t] 
            temp = np.multiply(temp,-1) #makes asd to des
            inds = temp.argsort()[:k] #smallest k numbers
            yP[u,inds,t] = 1

    prec = np.zeros(nT)
    reca = np.zeros(nT)
    for t in range(nT) :
        prec[t] , reca[t] = getPrecisionAndRecall(groundT[:,:,t],yP[:,:,t])

    avgPrec = np.sum(prec) / nT
    avgReca = np.sum(reca) / nT
    return avgPrec , avgReca

pre,rec = getCombinedPrecisionRecall(total,poiData.data,5,poiData)

print (rec,pre)